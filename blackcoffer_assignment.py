# -*- coding: utf-8 -*-
"""blackcoffer-assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YguVsojlHyT6P-EOXw8AqIY-Eyag0BV9
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import requests
import os
from bs4 import BeautifulSoup
import nltk
import string
nltk.download('stopwords')

df = pd.read_excel('/content/Input.xlsx')  

if not os.path.exists('extracted_text'):
  os.makedirs('extracted_text')

for index, row in df.iterrows():
   response = requests.get(row['URL'])
   html_content = response.content 
   def cont_str(j):
      post_content = ""
      for text in j:
        a =text.get_text()
        post_content += a
      return post_content

    

   soup = BeautifulSoup(html_content, 'html.parser')
   article_title = soup.find('h1', {'class' :'entry-title'})
   if article_title is not None:
     article_head = article_title.text
      # = article_title_text
    #  .translate(str.maketrans('', '', string.punctuation))
   else:
      article_title_text = ''

     

   article_text_element = soup.find(attrs={'class' :'td-post-content'})
   if article_text_element is not None:
    article_text = cont_str(article_text_element.find_all('p')[1:])
    article_content = article_text
   else:
     print(index ,row['URL_ID'])
     continue
     

  
  
 
   file_path = os.path.join('extracted_text', f"{row['URL_ID']}.txt")
   
   with open(file_path, 'w') as file:
    file.write(article_head + '\n\n' + article_content)

f = open("/content/extracted_text/37.0.txt").read()
len(f)

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def rempunc(d):
     punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~•“”'''
     no_punct = " "
     for char in d:
       if char not in punctuations:
         no_punct = no_punct + char.lower()
     return no_punct

"""STOPWORDS"""

stopwords = set(stopwords.words('english'))

os.chdir('/content/drive/MyDrive/assignment/StopWords/StopWords')

stop_words = []
for file_name in os.listdir():
    if file_name.endswith('.txt'):
        with open(file_name, 'r',encoding='ISO-8859-1') as f:
            stop_words.extend(f.read().splitlines())

# print(stop_words)

stop_words.extend(stopwords)
# len(stop_words)

text_dir = "/content/extracted_text/"

texts_inside_txt = []
 # print(os.path.join(text_dir,text_file))
for text_file in os.listdir(text_dir):
 
  with open(os.path.join(text_dir,text_file),'r') as f:  
    raw_t = rempunc(f.read().translate(str.maketrans('', '', string.punctuation)))
    
    text = raw_t.lower()


  
  word_tokens = word_tokenize(text)
  # print(len(text))

  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]

  filtered_sentence = []
  
  for w in word_tokens:
    if w not in stop_words:
      filtered_sentence.append(w)

  texts_inside_txt.append(filtered_sentence)

master_d = '/content/drive/MyDrive/assignment/MasterDictionary/MasterDictionary'

os.listdir(master_d)

positive_w = []
negative_w = []
for file_name in os.listdir(master_d):

    if file_name == 'positive-words.txt':
      with open(os.path.join(master_d,file_name), 'r',encoding='ISO-8859-1') as f:
         pos_word =f.read().splitlines()    
         positive_w.extend(pos_word)
        #  print(positive_w)

    else:
      with open(os.path.join(master_d,file_name), 'r',encoding='ISO-8859-1') as f:
         neg_word =f.read().splitlines()    
         negative_w.extend(neg_word)
        #  print(negative_w)

positive_words = []
Negative_words =[]
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []

#iterate through the list of docs
for i in range(len(texts_inside_txt)):
  positive_words.append([word for word in texts_inside_txt[i] if word.lower() in positive_w])
  Negative_words.append([word for word in texts_inside_txt[i] if word.lower() in negative_w])

  positive_score.append(len(positive_words[i]))
  negative_score.append(len(Negative_words[i]))
  
  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))
  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(texts_inside_txt[i])) + 0.000001))

avg_sentence_length = []
Percentage_of_Complex_words  =  []
Fog_Index = []
complex_word_count =  []
avg_syllable_word_count =[]

import re
def lenghth(file):
  with open(os.path.join(text_dir, file),'r') as f:
    text = f.read()

    text = re.sub(r'[^\w\s.]','',text)

    sentences = text.split('.')

    num_sentences = len(sentences)

    words = [word  for word in text.split() if word.lower() not in stopwords ]
    num_words = len(words)
 

    complex_words = []
    for word in words:
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word > 2:
        complex_words.append(word)


    syllable_count = 0
    syllable_words =[]
    for word in words:
      if word.endswith('es'):
        word = word[:-2]
      elif word.endswith('ed'):
        word = word[:-2]
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word >= 1:
        syllable_words.append(word)
        syllable_count += syllable_count_word


    avg_sentence_len = num_words / num_sentences
    avg_syllable_word_count = syllable_count / len(syllable_words)
    Percent_Complex_words  =  len(complex_words) / num_words
    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)

    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count


for file in os.listdir(text_dir):
  x,y,z,a,b = lenghth(file)
  avg_sentence_length.append(x)
  Percentage_of_Complex_words.append(y)
  Fog_Index.append(z)
  complex_word_count.append(a)
  avg_syllable_word_count.append(b)

def cleaned_words(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    text = re.sub(r'[^\w\s]', '' , text)
    words = [word  for word in text.split() if word.lower() not in stopwords]
    length = sum(len(word) for word in words)
    average_word_length = length / len(words)
  return len(words),average_word_length

word_count = []
average_word_length = []
for file in os.listdir(text_dir):
  x, y = cleaned_words(file)
  word_count.append(x)
  average_word_length.append(y)

def count_personal_pronouns(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    personal_pronouns = ["I", "we", "my", "ours", "us"]
    count = 0
    for pronoun in personal_pronouns:
      count += len(re.findall(r"\b" + pronoun + r"\b", text)) # \b is used to match word boundaries
  return count

pp_count = []
for file in os.listdir(text_dir):
  x = count_personal_pronouns(file)
  pp_count.append(x)

output_df = pd.read_excel('/content/OutputDataStructure.xlsx')


output_df.drop([44-37,57-37,144-37], axis = 0, inplace=True)


columns = [positive_score,
            negative_score,
            polarity_score,
            subjectivity_score,
            avg_sentence_length,
            Percentage_of_Complex_words,
            Fog_Index,
            avg_sentence_length,
            complex_word_count,
            word_count,
            avg_syllable_word_count,
            pp_count,
            average_word_length]


for i, p in enumerate(columns):
  output_df.iloc[:,i+2] = p

output_df.to_csv('/content/Output_Data.csv')

# output_df.groupby(output_df['POSITIVE SCORE'] > output_df['NEGATIVE SCORE']).get_group(True)

# output_df.head(20)

